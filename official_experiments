To see the billing information broken down by instance type:
https://console.aws.amazon.com/cost-reports/home#/custom?groupBy=InstanceType&hasBlended=false&hasAmortized=false&excludeDiscounts=true&excludeTaggedResources=false&timeRangeOption=Last12Months&granularity=Monthly&reportName=&reportType=CostUsage&isTemplate=true&startDate=2018-01-01&endDate=2018-12-31&filter=%5B%7B%22dimension%22:%22RecordType%22,%22values%22:%5B%22Refund%22,%22Credit%22%5D,%22include%22:false,%22children%22:null%7D%5D&forecastTimeRangeOption=None&usageAs=usageQuantity&chartStyle=Stack

Google cloud experiments - repeated many times.

Text:
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-highmem-64 text "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 2`; do python launch.py -k  -c -z n1-highmem-64 text_no_length_no_map_no_unfold "python text.py --noLength --noMap --noUnfold  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 text_mask "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --mask --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 text_mask "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --mask --ensembleSize 1 -RS 5000 --seed $SEED" ; done

for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-highmem-64 text_no_dsl "python text.py  -t 720  --pseudoCounts 30 --aic 1000.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 2 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-highmem-64 text_no_generative "python text.py  -t 720  --topK 2 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --ensembleSize 1 -RS 5000 --no-dsl --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 text_no_generative_mask "python text.py  -t 720  --topK 2 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --ensembleSize 1 -RS 5000 --no-dsl --mask --seed $SEED" ; done
for SEED in `seq 4 5`; do python launch.py -k  -c -z n1-highmem-64 text_no_recognition "python text.py -g   -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20  --storeTaskMetrics --testingTimeout 600 --taskReranker randomShuffle --taskBatchSize 10 --seed $SEED" ; done


List:
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-highmem-64 list "python list.py --split 0.5 -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 2`; do python launch.py -k  -c -z n1-highmem-64 list_no_length_no_map_no_unfold "python list.py --noLength --noMap --noUnfold --split 0.5 -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 2`; do python launch.py -k  -c -z n1-highmem-64 list_small_train "python list.py --split 0.25 -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 3 5`; do python launch.py -k  -c -z n1-highmem-64 list_tiny_train "python list.py --split 0.15 -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 2`; do python launch.py -k  -c -z n1-highmem-64 list_small_train_no_dsl "python list.py --no-dsl --split 0.25 -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-highmem-64 list_tiny_train_no_dsl "python list.py --no-dsl --split 0.15 -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 list_mask "python list.py --split 0.5 -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --mask --seed $SEED" ; done
for SEED in `seq 4 5`; do python launch.py -k  -c -z n1-highmem-64 list_no_dsl_no_ensemble "python list.py --aic 1000.0 --split 0.5 -t 720  --pseudoCounts 30 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 list_no_generative "python list.py --split 0.5 -t 720 --topK 2 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --no-dsl --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 1`; do python launch.py -k  -c -z n1-highmem-64 list_no_generative_no_batch "python list.py --split 0.5 -t 7200 --topK 2 --maximumFrontier 5 -i 5 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --no-dsl --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 list_no_generative_mask "python list.py --split 0.5 -t 720 --topK 2 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --no-dsl --ensembleSize 1 -RS 5000 --mask --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 list_no_generative_old_recognition "python list.py --split 0.5 -t 720 --topK 2 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --taskReranker randomShuffle --taskBatchSize 10 --no-dsl --ensembleSize 1 -RS 5000 --seed $SEED" ; done
for SEED in `seq 4 5`; do python launch.py -k  -c -z n1-highmem-64 list_no_recognition "python list.py  -g --split 0.5 -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --taskReranker randomShuffle --taskBatchSize 10 --seed $SEED" ; done

LOGO:
for SEED in `seq 1 1`; do python launch.py -k  -c -z n1-megamem-96 logo_no_batch_2h "python logo.py --split 0.5 -t 7200  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 6 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual  -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-megamem-96 logo_batch_40_1h "python logo.py --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 6 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual  --taskReranker randomShuffle --taskBatchSize 40 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-megamem-96 logo_auxiliary_batch_40_1h "python logo.py --auxiliary --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual  --taskReranker randomShuffle --taskBatchSize 40 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 1`; do python launch.py -k  -c -z n1-megamem-96 logo_auxiliary_no_batch_2h "python logo.py --split 0.5 -t 7200  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 6 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual  -RS 5000 --seed $SEED --auxiliary" ; done
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-megamem-96 logo_no_dsl_auxiliary_batch_40_1h "python logo.py --no-dsl --auxiliary --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --storeTaskMetrics --testingTimeout 600 --biasOptimal --contextual  --taskReranker randomShuffle --taskBatchSize 40 -RS 5000 --seed $SEED" ; done
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-megamem-96 logo_no_recognition_batch_40_1h "python logo.py -g  --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 11  --storeTaskMetrics --testingTimeout 600  --taskReranker randomShuffle --taskBatchSize 40 --seed $SEED" ; done

Scientific laws:
python launch.py -k  -c -z n1-highmem-64 scientific_1h "python scientificLaws.py  -t 3600 --topK 5 --arity 3 --maximumFrontier 5 -i 10 -R 3600 -RS 5000 --biasOptimal --contextual --mask  -r 0."

REGEX:
Reduced Primitives:
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_reduced_mask "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_reduced_bigram "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_reduced_unigram "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_reduced_nodsl_mask "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --no-dsl --seed $SEED" ; done
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_reduced_nodsl_mask "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --no-dsl --seed $SEED" ; done
#for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_reduced_norec "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --g --seed $SEED" ; done

for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_reduced_nodsl_bigram "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --no-dsl --seed $SEED" ; done

for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_reduced_nodsl_unigram "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --no-dsl --seed $SEED" ; done


for SEED in `seq 1 1`; do python launch.py -k  -c -z n1-highmem-64 regex_2h_mask "python regexes.py --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 7200  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --mask --auxiliary --seed $SEED" ; done
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-highmem-64 regex_batch_40_mask "python regexes.py --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --seed $SEED" ; done
No DSL baseline:
Need to run:
for SEED in `seq 1 5`; do python launch.py -k  -c -z n1-highmem-64 regex_no_dsl_batch_40_mask "python regexes.py --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --no-dsl --seed $SEED" ; done
noRecognition:
python launch.py -k  -c -z n1-highmem-64 test "python regexes.py --tasks new --taskReranker randomShuffle --taskBatchSize 40 --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 --testingTimeout 1800 --seed 0 -g" 
for SEED in `seq 1 2`; do python launch.py -k  -c -z n1-highmem-64 test "python regexes.py --tasks new --taskReranker randomShuffle --taskBatchSize 40 --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 --testingTimeout 1800 --seed $SEED -g" ; done
Need to do:
for SEED in `seq 3 5`; do python launch.py -k  -c -z n1-highmem-64 test "python regexes.py --tasks new --taskReranker randomShuffle --taskBatchSize 40 --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 20 --testingTimeout 1800 --seed $SEED -g" ; done

#1 hour checkpoint:
python launch.py --checkpoint experimentOutputs/regex/2019-02-12T21:51:03.349621/regex_aic=1.0_arity=3_aux=True_BO=True_CO=True_ES=1_ET=3600_HR=0.5_it=10_mask=True_MF=10_pc=30.0_RT=3600_RR=False_RW=False_STM=True_L=1.5_batch=40_TRR=randomShuffle_K=2_topkNotMAP=True.pickle -k -c -z n1-highmem-64 regex_batch_40_reduced_mask_checkpoint "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 11 -R 3600 --testingTimeout 7200 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --seed 0 --resume experimentOutputs/regex_aic=1.0_arity=3_aux=True_BO=True_CO=True_ES=1_ET=3600_HR=0.5_it=10_mask=True_MF=10_pc=30.0_RT=3600_RR=False_RW=False_STM=True_L=1.5_batch=40_TRR=randomShuffle_K=2_topkNotMAP=True.pickle"

python launch.py --checkpoint experimentOutputs/regex/2019-02-12T21:51:03.349621/regex_aic=1.0_arity=3_aux=True_BO=True_CO=True_ES=1_ET=3600_HR=0.5_it=10_mask=True_MF=10_pc=30.0_RT=3600_RR=False_RW=False_STM=True_L=1.5_batch=40_TRR=randomShuffle_K=2_topkNotMAP=True.pickle -k -c -z n1-highmem-64 regex_batch_40_reduced_mask_noC "python regexes.py --primitives reduced --tasks new --maxTasks 256 --ll_cutoff bigram None --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 11 -R 3600 --testingTimeout 7200 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --seed 0 --resume experimentOutputs/regex_aic=1.0_arity=3_aux=True_BO=True_CO=True_ES=1_ET=3600_HR=0.5_it=10_mask=True_MF=10_pc=30.0_RT=3600_RR=False_RW=False_STM=True_L=1.5_batch=40_TRR=randomShuffle_K=2_topkNotMAP=True.pickle"

#fixed bigram posterior
for SEED in `seq 1 3`; do python launch.py -k  -c -z n1-highmem-64 regex_b40_mask_posterior "python regexes.py --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 3600 --testingTimeout 1800 --biasOptimal --contextual --mask --auxiliary --taskReranker randomShuffle --taskBatchSize 40 --seed $SEED" ; done

TOWER
for SEED in `seq 1 3`; do python launch.py -c  -k -z n1-highmem-64 tower_random_shuffle_10_120s "python tower.py -t 120  --pseudoCounts 30 --tasks old --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --primitives old --recognitionTimeout 3600 -RS 5000 --seed $SEED" ; done # did not do well

python launch.py -c  -k -z n1-highmem-64 tower_no_batch_1h "python tower.py -t 3600  --pseudoCounts 30 --tasks new --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --primitives new --recognitionTimeout 3600 -RS 5000" # does very well! both quantitatively and qualitatively

python launch.py -c  -k -z n1-highmem-64 tower_batch_10_720 "python tower.py -t 720  --pseudoCounts 30 --tasks new --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 10 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --primitives new --recognitionTimeout 3600 -RS 5000"
for SEED in `seq 1 4`; do python launch.py -c  -k -z n1-highmem-64 tower_batch_40_3600 "python tower.py -t 3600  --pseudoCounts 30 --tasks new --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 11 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 40 --primitives new --recognitionTimeout 3600 -RS 5000 --seed $SEED" ; done
for SEED in `seq 0 4`; do python launch.py -c  -k -z n1-highmem-64 tower_no_dsl_batch_40_3600 "python tower.py --no-dsl -t 3600  --pseudoCounts 30 --tasks new --maximumFrontier 5 -i 11 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 40 --primitives new --recognitionTimeout 3600 -RS 5000 --seed $SEED" ; done
for SEED in `seq 0 4`; do python launch.py -c  -k -z n1-highmem-64 tower_no_recognition_batch_40_3600 "python tower.py -g  -t 3600  --pseudoCounts 30 --tasks new --aic 1.0 --structurePenalty 1.5 --topK 2 --arity 3 --maximumFrontier 5 -i 11 --storeTaskMetrics --split 0.5 --testingTimeout 600  --taskReranker randomShuffle --taskBatchSize 40 --primitives new  --seed $SEED" ; done


The hyper parameters here are not consistent. A key thing we need to
do is make sure that we are always using the same hyper parameters, or
if we are not, we need a good reason for it. For example, the
structure penalty on text is larger than for the other domains, but
this is justifiable. And the top K for regular expressions is larger
than for the other domains but that is also justifiable. In contrast
the pseudocounts really should be the same everywhere.

LOGO:
Full model without batching:
     python launch.py -k  -z x1.32xlarge DreamLogo "python logo.py -t 7200 --structurePenalty 1.5 --pseudoCounts 30.0 --biasOptimal --contextual --split 0.5 --testingTimeout 3600"
Random shuffle (batches of 10), 720s: [DONE, solves 36/73 testing]
     python launch.py -k  -z r4.16xlarge LogoBatch "python logo.py -t 720  --structurePenalty 1.5 --pseudoCounts 30.0 --biasOptimal --contextual --split 0.5 --testingTimeout 3600  --storeTaskMetrics --taskReranker randomShuffle --taskBatchSize 10 -i 20  -R 1800 --reuseRecognition"
Random shuffle (batches of 10), 720s:  [DONE: solves 40/73 and ends up converging to a bad DSL]
     python launch.py -k  -z x1.32xlarge LogoBatch_24m "python logo.py -t 1440  --structurePenalty 1.5 --pseudoCounts 30.0 --biasOptimal --contextual --split 0.5 --testingTimeout 3600  --storeTaskMetrics --taskReranker randomShuffle --taskBatchSize 10 -i 30  -R 1800 "
Random shuffle (batches of 10), 3600s:  [DONE: solves 40/73 and ends up converging to a bad DSL]
     python launch.py -k  -z x1.32xlarge logo_batch_10_1h "python logo.py -t 3600  --structurePenalty 1.0 --pseudoCounts 30.0 --biasOptimal --contextual --split 0.5 --testingTimeout 600  --storeTaskMetrics --taskReranker randomShuffle --taskBatchSize 10 -i 30  -R 1800 "
Random shuffle (batches of 20), 30min: [DONE: solves 50/73 and still converges to a bad DSL]
     python launch.py -k  -z x1.32xlarge logo_batch_20_30m "python logo.py -t 1800  --structurePenalty 1.5 --pseudoCounts 30.0 --biasOptimal --contextual --split 0.5 --testingTimeout 600  --storeTaskMetrics --taskReranker randomShuffle --taskBatchSize 20 -i 30  -R 1800 "
Random shuffle (batches of 40), 60min: [DONE: solves almost 90%, slightly worse than no batching]
            python launch.py -k  -z x1.32xlarge logo_batch_40_60m "python logo.py -t 3600  --structurePenalty 1.5 --pseudoCounts 30.0 --biasOptimal --contextual --split 0.5 --testingTimeout 600  --storeTaskMetrics --taskReranker randomShuffle --taskBatchSize 40 -i 20  -R 1800 "
*old recognition*, Random shuffle (batches of 40), 60min: [RUNNING]
            python launch.py -k  -z x1.32xlarge logo_batch_40_60m_old_recognition "python logo.py -t 3600  --structurePenalty 1.5 --pseudoCounts 30.0 --biasOptimal --contextual --split 0.5 --testingTimeout 600  --storeTaskMetrics --taskReranker randomShuffle --taskBatchSize 40 -i 20  -R 3600 "

Baseline without recognition model (batches of 40), 720: [RUNNING]
            python launch.py -k  -z x1.32xlarge logo_no_recognition_batch_40_60m "python logo.py -t 3600  --structurePenalty 1.5 --pseudoCounts 30.0 --split 0.5 --testingTimeout 600 --taskReranker randomShuffle --taskBatchSize 40 -i 20  -g"
Baseline without DSL learning (batches of 40): [RUNNING]	    
            python launch.py -k  -z x1.32xlarge logo_no_dsl_batch_40_60m "python logo.py -t 3600  --structurePenalty 1.5 --pseudoCounts 30.0 --biasOptimal --contextual --split 0.5 --testingTimeout 600  --storeTaskMetrics --taskReranker randomShuffle --taskBatchSize 40 -i 20  -R 1800 --aic 1000.0"


TEXT:
Full model without batching:
     python launch.py -k  -z x1.32xlarge TextDream "python text.py  -i 6 -t 7200 --pseudoCounts 30 --testingTimeout 1800  --contextual --biasOptimal -l 5 --maximumFrontier 2"


Derby: (You can replace this checkpoint with a different one and it will run the Derby with it)
	python launch.py -k  -z c4.8xlarge --checkpoint experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.5_it=19_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle  batched_derby "python text.py --compete experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.5_it=19_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle"
	python launch.py -k  -z c4.8xlarge --checkpoint experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.5_it=20_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle  batched_derby_it20 "python text.py --compete experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.5_it=20_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle"

Baseline without batching, 3600s: text_baseline_3600s [Done: solves up to 80 tasks.]
	python launch.py  -k -z r4.16xlarge text_baseline_3600s "python text.py  -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 7 -R 7200 --storeTaskMetrics --testingTimeout 3600 --biasOptimal --contextual --taskReranker default"

Baseline without batching, 7200: text_baseline_7200s	[Done: solves 87 tasks.]
	python launch.py  -k -z r4.16xlarge text_baseline_7200s "python text.py  -t 7200  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 7 -R 7200 --storeTaskMetrics --testingTimeout 7200 --biasOptimal --contextual --taskReranker default"

Baseline without recognition model (batches of 10), 720s: [DONE]
	python launch.py  -k -z r4.16xlarge text_no_recognition_random_shuffle_10_720s "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20  --testingTimeout 600  -g  --taskReranker randomShuffle --taskBatchSize 10"

Baseline without DSL learning (batches of 10), 720s: [DONE]
	python launch.py  -k -z r4.16xlarge text_no_dsl_random_shuffle_10_720s "python text.py  -t 720  --pseudoCounts 30 --aic 1000.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"

Baseline without DSL learning + old recognition model (batches of 10), 720s: [RUNNING]
	python launch.py  -k -z r4.16xlarge text_no_dsl_old_recognition_random_shuffle_10_720s "python text.py  -t 720  --pseudoCounts 30 --aic 1000.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --taskReranker randomShuffle --taskBatchSize 10"
	
Random shuffle (batches of 10), 720s: text_random_shuffle_10_720s [DONE - solves 91.]
	python launch.py  -k -z r4.16xlarge text_random_shuffle_10_720s "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"

Random shuffle (batches of 40), 3600s: text_random_shuffle_40_3600s [RUNNING]
	python launch.py  -k -z r4.16xlarge text_random_shuffle_40_3600s "python text.py  -t 3600  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 40"


Random shuffle (batches of 10), 720s, structure penalty: text_random_shuffle_10_720s_sp [Done - solves 56/108.]
	python launch.py  -k -z r4.16xlarge text_random_shuffle_10_720s_sp "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"


Random shuffle (batches of 10), 720s, more dreams: text_random_shuffle_10_720s_r [Done - solves 59/108.]
	python launch.py  -k -z r4.16xlarge text_random_shuffle_10_720s_r "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 -r 0.9"


Random shuffle_r: resuming (text_random_shuffle_10_720s_r) with more time: *text_resume_1440s
	python launch.py  -k --ssh_key openmind -z r4.16xlarge --checkpoint experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.9_it=20_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle text_resume_1440s "python text.py  -t 1440  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 31 -R 3600 --storeTaskMetrics --testingTimeout 1440 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 -r 0.9 --resume experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.9_it=20_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle"

Random shuffle_r: resuming (text_random_shuffle_10_720s_r) with more time on just unsolved: *text_resume_unsolved_1440s
python launch.py  -k --ssh_key openmind -z r4.16xlarge --checkpoint experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.9_it=20_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle text_resume_unsolved_1440s "python text.py  -t 1440  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 31 -R 3600 --storeTaskMetrics --testingTimeout 1440 --biasOptimal --contextual --taskReranker unsolved --taskBatchSize 10 -r 0.9 --resume experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.9_it=20_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle"

Random shuffle_r: resuming (text_random_shuffle_10_720s_r) with more time on just unsolved: *text_resume_unsolved_2160s
python launch.py  -k --ssh_key openmind -z r4.16xlarge --checkpoint experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.9_it=20_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle text_resume_unsolved_2160s "python text.py  -t 2160  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 31 -R 3600 --storeTaskMetrics --testingTimeout 2160 --biasOptimal --contextual --taskReranker unsolved --taskBatchSize 10 -r 0.9 --resume experimentOutputs/text_aic=1.0_arity=3_BO=True_CO=True_ET=720_HR=0.9_it=20_MF=5_baseline=False_pc=30.0_RT=7200_RW=False_storeTask=True_L=1.0_batch=10_taskReranker=randomShuffle_K=2_topkNotMAP=False_rec=True_feat=LearnedFeatureExtractor.pickle"


Random shuffle (batches of 20), 1440s: text_random_shuffle_20_1440s [DONE - solves 56 tasks.]
	python launch.py  -k -z r4.16xlarge text_random_shuffle_20_1440s "python text.py  -t 1440  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 7200 --storeTaskMetrics --testingTimeout 1440 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 20"

Random kNN (batches of 10), 720s: test_random_knn_10_720s [Done: solves up to 87 tasks.]
	python launch.py  -k -z r4.16xlarge test_random_knn_10_720s "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --biasOptimal --contextual --taskReranker randomkNN --taskBatchSize 10"

Unsolved (ranked by entropy, batches of 10), 720s: text_unsolved_entropy_10_720s [Done - solves up to 91 tasks.]
	python launch.py  -k -z r4.16xlarge text_unsolved_entropy_10_720s "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --biasOptimal --contextual --taskReranker unsolvedEntropy --taskBatchSize 10"

Curriculum (batch size 10), 720s: text_default_10_720s [Done - solves up to 91 tasks.]
	python launch.py  -k -z r4.16xlarge text_default_10_720s "python text.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --testingTimeout 720 --biasOptimal --contextual --taskReranker default --taskBatchSize 10"


LIST:
1/18 Replication Runs: # Cap 10,000 gradient steps.

Best full model, random shuffle (batches of 10), structure penalty, no vectorization: *list_random_shuffle_10_720s_sp_no_vec_1: replication 1. [Best 103/109]
	python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_no_vec_1 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 10000 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"

	Replication 2: Done. Best 90/109.
	python launch.py  -k -z r4.16xlarge *list_random_shuffle_10_720s_sp_no_vec_2 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 10000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"

	Replication 3: *list_random_shuffle_10_720s_sp_no_vec_3 Done. Best 84/109.

Best full model, random shuffle (batches of 10), structure penalty, vectorization with gradient steps capped. Best: 104/109.
	python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_vec_1 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 10000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 "

	Done: best 103/109.
	python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_vec_2 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 10000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 "


Best full model, no vectorization, auxiliary loss. Best: 88/109.
		python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_no_vec_aux_1 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 10000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --auxiliary"

		Running one replication. Best: 104/109.

Best full model, vectorization, auxiliary loss. Best: 104/109.
		python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_vec_aux_1 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 10000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary"


Best full model, vectorization with many steps (sanity check.)
python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_vec_many_1 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 150000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 "

python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_vec_many_2 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 150000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 "


Best full model, vectorization with gradient steps capped, ensemble = 3.
python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_vec_ensemble_1 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 10000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --ensembleSize 3"

Replication: list_random_shuffle_10_720s_sp_vec_ensemble_2 

Best full model, vectorization, auxiliary loss, ensemble = 3.
python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_vec_aux_ensemble_1 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -RS 10000  --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10  --auxiliary --ensembleSize 3"

Repython launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp_vec_aux_ensemble_2.


---
Full model without batching:
     python launch.py -k  -z x1.32xlarge ListDream "python list.py -t 7200 --split 0.5 --testingTimeout 600 --contextual --biasOptimal -i 6 --maximumFrontier 5 --pseudoCounts 10. --structurePenalty 2"

Full model without batching, 3600s timeout: list_baseline_3600 [DONE: 103/109.]
	python launch.py  -k -z x1.32xlarge list_baseline_3600 "python list.py  -t 3600  --pseudoCounts 10 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 10 -i 7 -R 3600 --storeTaskMetrics --split 0.5 --testingTimeout 3600 --biasOptimal --contextual --taskReranker default"	

Full model without batching, 7200s timeout: list_baseline_7200s [DONE - Solves 109/109 tasks.]
	python launch.py  -k -z x1.32xlarge list_baseline_7200 "python list.py  -t 7200  --pseudoCounts 10 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 10 -i 7 -R 3600 --storeTaskMetrics --split 0.5 --testingTimeout 7200 --biasOptimal --contextual --taskReranker default"

Random shuffle (batches of 10), 720s: list_random_shuffle_10_720s  [Done: Solves 101/109]
	python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"

Baseline without DSL learning (batches of 10), 720s: [RUNNING]
	python launch.py  -k -z r4.16xlarge list_no_dsl_random_shuffle_10_720s "python list.py  -t 720  --pseudoCounts 30 --aic 1000.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"

Baseline without DSL learning (batches of 10), 720s: [RUNNING]
	python launch.py  -k -z r4.16xlarge list_no_dsl_old_recognition_random_shuffle_10_720s "python list.py  -t 720  --pseudoCounts 30 --aic 1000.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 720  --taskReranker randomShuffle --taskBatchSize 10"


Baseline without recognition model (batches of 10), 720s: list_no_recognition_random_shuffle_10_720s  [RUNNING]
	python launch.py  -k -z r4.16xlarge list_no_recognition_random_shuffle_10_720s "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 --split 0.5 --testingTimeout 600 -g  --taskReranker randomShuffle --taskBatchSize 10"

Random shuffle (batches of 10), structure penalty: list_random_shuffle_10_720s_sp [Done: solves 105/109.]
	python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_sp "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"
Running for replication: *list_random_shuffle_10_720s_sp_v2
		python launch.py --ssh_key openmind -k -z r4.16xlarge list_random_shuffle_10_720s_sp_v2 "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"

Random shuffle (batches of 10), more dreams: list_random_shuffle_10_720s_r [Done: solves 104/109.]
	python launch.py  -k -z r4.16xlarge list_random_shuffle_10_720s_r "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 -r 0.9"

Random shuffle (batches of 20), 1440s: list_random_shuffle_20_1440s  [DONE: solves 102/109.]
	python launch.py  -k -z r4.16xlarge list_random_shuffle_20_1440s "python list.py  -t 1440  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 10 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 1440 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10"


Random shuffle (batches of 10), structure penalty, retraining: list_random_shuffle_10_720s_sp_retrain [DONE: solves 85/109].
	python launch.py  -k -z r4.16xlarge list_random_shuffle
	e_10_720s_sp_retrain "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 1800 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --reuseRecognition"


Random kNN (batches of 10), 720s: list_random_knn_10_720s [Done - solves 84/109]
	python launch.py  -k -z r4.16xlarge list_random_knn_10_720s_catwong  "python list.py  -t 720  --pseudoCounts 30 --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomkNN --taskBatchSize 10"

Random kNN (batches of 10), regularized: list_random_knn_10_720s_reg [Done - solves 105/109.]
	python launch.py  -k -z r4.16xlarge list_random_knn_10_720s_reg  "python list.py  -t 720 --pseudoCounts 30 --aic 1.0 --structurePenalty 2.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 7200 --storeTaskMetrics --split 0.5 --testingTimeout 720 --biasOptimal --contextual --taskReranker randomkNN --taskBatchSize 10 -r 0.9"





TOWER:
Full model without batching: (Solves up to 51 tasks)
     python launch.py -k -z m4.16xlarge TowerDream5 "python tower.py -i 6 -t 300 --pseudoCounts 30 --tasks old --maximumFrontier 5  --biasOptimal --contextual --testingTimeout 600 --split 0.5 --structurePenalty 1"

Full model without batching and without test/train split, new primitives: [RUNNING]
     python launch.py -k -z m4.16xlarge tower_new_everything "python tower.py -i 6 -t 300 --pseudoCounts 30 --tasks old --maximumFrontier 5  --biasOptimal --contextual  --structurePenalty 1 --primitives new"
     
Full model without batching, new primitives: [Done, solves 50/56]
     python launch.py -k -z m4.16xlarge tower_new "python tower.py --recognitionTimeout 3600 -i 6 -t 3600 --pseudoCounts 30 --tasks old --maximumFrontier 5  --biasOptimal --contextual --testingTimeout 600 --split 0.5 --structurePenalty 1 --primitives new"

Random shuffle batches of 10, new primitives: [Done, solves 44/56]
     python launch.py -k -z m4.16xlarge tower_new_random_shuffle_10_360s "python tower.py  --recognitionTimeout 3600 -i 20 -t 120 --pseudoCounts 30 --tasks old --maximumFrontier 5  --biasOptimal --contextual --testingTimeout 600 --split 0.5 --structurePenalty 1 --primitives new --taskReranker randomShuffle --taskBatchSize 10"

Random shuffle batches of 10, 720s, new primitives: [RUNNING]
     python launch.py -k -z m4.16xlarge tower_new_random_shuffle_10_720s "python tower.py  --recognitionTimeout 3600 -i 20 -t 720 --pseudoCounts 30 --tasks old --maximumFrontier 5  --biasOptimal --contextual --testingTimeout 600 --split 0.5 --structurePenalty 1 --primitives new --taskReranker randomShuffle --taskBatchSize 10"


Random shuffle (batches of 5), 60s: tower_random_shuffle_5_60s [Done, solves 42/56]
	python launch.py -k -z m4.16xlarge tower_random_shuffle_5_60s "python tower.py -t 60  --pseudoCounts 30 --tasks old --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 300 --storeTaskMetrics --split 0.5 --testingTimeout 60 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 5"

Random shuffle (batches of 5), 60s: tower_random_shuffle_5_120s [Done, solves 45/56].
	python launch.py -k -z m4.16xlarge tower_random_shuffle_5_120s "python tower.py -t 120  --pseudoCounts 30 --tasks old --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 300 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 5"

Random shuffle (batches of 10), 120s: tower_random_shuffle_10_120s [Done, solves 51/56]
	python launch.py -k -z m4.16xlarge tower_random_shuffle_10_120s "python tower.py -t 120  --pseudoCounts 30 --tasks old --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --primitives old --recognitionTimeout 3600"
			Re-run with new random shuffle (v2): solves 49/56.

Baseline without recognition model: tower_no_recognition_random_shuffle_10_120s [RUNNING]
	python launch.py -k -z m4.16xlarge tower_no_recognition_random_shuffle_10_120s "python tower.py -t 120  --pseudoCounts 30 --tasks old --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 --storeTaskMetrics --split 0.5 --testingTimeout 600 --taskReranker randomShuffle --taskBatchSize 10 --primitives old -g"

Baseline without DSL learning: tower_no_dsl_random_shuffle_10_120s [RUNNING]
	python launch.py -k -z m4.16xlarge tower_no_dsl_random_shuffle_10_120s "python tower.py -t 120  --pseudoCounts 30 --tasks old --aic 1000.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --primitives old --recognitionTimeout 3600"

Random shuffle (batches of 10), 120s. retraining: tower_random_shuffle_10_120s [12/18]
	python launch.py -k -z m4.16xlarge --ssh_key openmind tower_random_shuffle_10_120s "python tower.py -t 120  --pseudoCounts 30 --tasks old --aic 1.0 --structurePenalty 1.0 --topK 2 --arity 3 --maximumFrontier 5 -i 20 -R 300 --storeTaskMetrics --split 0.5 --testingTimeout 600 --biasOptimal --contextual --taskReranker randomShuffle --taskBatchSize 10 --reuseRecognition"

REGEX:
Best run so far (uses context free):
python launch.py -g -z "p3.16xlarge" "regex_gpu_contextfreeBO.95HR" "python regexes.py -t 3600 --testingTimeout 1800 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --biasOptimal -r .95"

Best contextual run:
python launch.py -g -z "p3.16xlarge" "regex_gpu_contextualBO.95HR" "python regexes.py -t 3600 --testingTimeout 1800 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r .95"

A task batching run
python launch.py -g -z "p3.16xlarge" "regex_gpu_batch_contextualBO.95HR" "python regexes.py -t 720 --testingTimeout 720 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r .95 --taskReranker randomShuffle --taskBatchSize 10"

task batchinng with rec model reuse:

python launch.py -g -z "p3.16xlarge" "regex_gpu_batch_reuserec_contextualBO.95HR" "python regexes.py -t 720 --testingTimeout 720 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r .95 --taskReranker randomShuffle --taskBatchSize 10 --reuseRecognition"

***
#100% helmholtz:
python launch.py -g -z "g3.16xlarge" "regex_gpu_batch_100HR" "python regexes.py -t 720 --testingTimeout 720 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r 1 --taskReranker randomShuffle --taskBatchSize 10"

#can use --seed to change subset of tasks. default is 42

#Reuse vs not reuse
python launch.py -g -z "g3.16xlarge" "regex_batch_.95HR" "python regexes.py -t 720 --testingTimeout 720 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r .95 --taskReranker randomShuffle --taskBatchSize 10"

#task batchinng with rec model reuse:

python launch.py -g -z "g3.16xlarge" "regex_batch_reuserec.95HR" "python regexes.py -t 720 --testingTimeout 720 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r .95 --taskReranker randomShuffle --taskBatchSize 10 --reuseRecognition"


LONGER RUNS, NORMAL STATS

python launch.py -g -z "g3.16xlarge" "regex_batch_50HR" "python regexes.py -i 30 -t 720 --testingTimeout 900 -R 1800 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r .5 --taskReranker randomShuffle --taskBatchSize 10"

python launch.py -g -z "g3.16xlarge" "regex_batch_reuserec50HR" "python regexes.py -i 30 -t 720 --testingTimeout 900 -R 1800 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r .5 --taskReranker randomShuffle --taskBatchSize 10 --reuseRecognition"




Full model with batching, 0.5 Helmholtz ratio, long recognition model train time, bounded rank of 32: [RUNNING]
python launch.py -g -z "g3.16xlarge" "regex_batch_40_1800s_r32" "python regexes.py -t 1800 --testingTimeout 1800 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 --contextual --biasOptimal -r .5 --taskReranker randomShuffle --taskBatchSize 40 --recognitionTimeout 1800 --reuseRecognition --matrixRank 32"

Baseline without recognition model:
python launch.py -k -z m4.16xlarge --tail regex_no_recognition_batch_40_1800 "python regexes.py -t 1800 --testingTimeout 1800 --tasks new --maxTasks 256 --ll_cutoff bigram --split 0.5 -k 10 -g --taskReranker randomShuffle --taskBatchSize 40 -i 20"